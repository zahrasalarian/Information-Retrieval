{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "from numpy.core.numeric import NaN\n",
    "from regex.regex import escape\n",
    "from collections import Counter\n",
    "import regex, copy, heapq, math, operator, random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys,os,sys\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install hazm\n",
    "from hazm import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_links = pd.read_csv('IR00_3_11k News.csv')\n",
    "docs_links = docs_links[docs_links['content'].notnull()]\n",
    "\n",
    "punctuations = pd.read_csv(\"punctuations.csv\")\n",
    "punctuations_to_remove = ''.join(punctuations[\"punctuations\"].values.tolist()).replace(\" \",\"\")\n",
    "stopwords_to_remove = ''.join(stopwords_list()).replace(\" \",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [pd.read_csv('IR00_3_11k News.csv'), pd.read_csv('IR00_3_17k News.csv'), pd.read_csv('IR00_3_20k News.csv')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "offset=int(files[0].tail(1)[\"id\"])\n",
    "files[1][\"id\"]=files[1][\"id\"]+offset\n",
    "offset=int(files[1].tail(1)[\"id\"])\n",
    "files[2][\"id\"]=files[2][\"id\"]+offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('IR00_3_11k News.csv')\n",
    "data[\"topic\"].replace({\"political\": \"politics\", \"sport\": \"sports\"}, inplace=True)\n",
    "data = pd.concat(files)\n",
    "data.index = data.id\n",
    "data = data[data['content'].notnull()]\n",
    "docs_links = data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = Normalizer()\n",
    "lemmatizer = Lemmatizer()\n",
    "docs_len = {}\n",
    "\n",
    "def tokenize(id, document):\n",
    "    document = str(document)\n",
    "    document = document.replace(\"انتهای پیام\",\"\")\n",
    "    n_document = str(normalizer.normalize(document))\n",
    "    #remove punctuations\n",
    "    n_document = n_document.translate(str.maketrans('','',punctuations_to_remove))\n",
    "    #remove stop words\n",
    "    tokens = list(map(lambda t:lemmatizer.lemmatize(t),word_tokenize(n_document))) \n",
    "    if id is not None:\n",
    "        docs_len[id] = len(tokens)\n",
    "\n",
    "    dictionary = dict(Counter(tokens))\n",
    "    for k, tf in dictionary.items():\n",
    "        dictionary[k] = 1 + math.log10(tf)\n",
    "    \n",
    "    if id is not None:\n",
    "        docs_len[id] = dictionary\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Weighted Inverted Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "N=len(docs_links[\"content\"])\n",
    "def build_weighted_inv_ind(docs_links):\n",
    "    dictionary = {}\n",
    "    for id, doc in zip(docs_links[\"id\"], docs_links[\"content\"]):\n",
    "        counts = tokenize(id, doc)\n",
    "        for k,v in counts.items():\n",
    "            if len(k) < 3:\n",
    "                continue\n",
    "            if k in dictionary.keys():\n",
    "                dictionary[k][0] += 1\n",
    "                dictionary[k][1][id] = v\n",
    "            else:\n",
    "                dictionary[k]=[1,{id:v}]\n",
    "    # calc idfs \n",
    "    for k in dictionary.keys():\n",
    "        dictionary[k][0] = math.log10(N/dictionary[k][0])\n",
    "    \n",
    "    for id, dl in docs_len.items():\n",
    "        size_d = 0\n",
    "        for t, w in dl.items():\n",
    "            if t in dictionary:\n",
    "                size_d += dictionary[t][0]*w\n",
    "        docs_len[id] = math.sqrt(size_d)\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Champion Lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_champion_lists(weighted_inv_ind):\n",
    "    champion_lists = {}\n",
    "    for t, ws in weighted_inv_ind.items():\n",
    "        champion_lists[t] = []\n",
    "        champion_lists[t].append(ws[0])\n",
    "        champion_lists[t].append(dict(sorted(ws[1].items(), key=operator.itemgetter(1), reverse=True)[:10]))\n",
    "    return champion_lists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 56.1 s, sys: 634 ms, total: 56.7 s\n",
      "Wall time: 56.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "weighted_inv_ind = build_weighted_inv_ind(docs_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build document term frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_doc_term_feq(docs_links):\n",
    "    dictionary = {}\n",
    "    for id, doc in zip(docs_links[\"id\"], docs_links[\"content\"]):\n",
    "        counts = tokenize(None, doc)\n",
    "        dictionary[id] = [sum(counts.values()), counts]\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_term_feq = build_doc_term_feq(docs_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "N=len(docs_links[\"content\"])\n",
    "def compute_similarity(nd_q, weighted_inv_ind):\n",
    "    similarities = {}\n",
    "    for k,v in nd_q.items():\n",
    "        if k not in weighted_inv_ind:\n",
    "            continue\n",
    "        idf = weighted_inv_ind[k][0]\n",
    "        for docid, w in weighted_inv_ind[k][1].items():\n",
    "            if docid in similarities.keys():\n",
    "                similarities[docid] += idf*w*v\n",
    "            else:\n",
    "                similarities[docid] = idf*w*v\n",
    "    for doc_id, sim in similarities.items():\n",
    "        similarities[doc_id] = sim/docs_len[doc_id]\n",
    "    return similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_kmeans(doc_term_freq):\n",
    "    sims_dict = {}\n",
    "    initial_centers = []\n",
    "    for _ in range(10):\n",
    "        initial_centers.append(doc_term_freq[random.randint(1,N+1)][1])\n",
    "    \n",
    "    clusters = {}\n",
    "    for _ in range(30):\n",
    "        clusters = {}\n",
    "        for i in range(len(initial_centers)):\n",
    "            sims = compute_similarity(initial_centers[i], weighted_inv_ind)\n",
    "            for doc_id, sim in sims.items():\n",
    "                sims_dict[(i, doc_id)] = sim\n",
    "        \n",
    "        # clustering\n",
    "        doc_cluster = {}\n",
    "        for ids in sims_dict.keys():\n",
    "            max_sim = -math.inf\n",
    "            max_cen = None\n",
    "            for i in range(len(initial_centers)):\n",
    "                cen_id = i\n",
    "                if (cen_id, ids[1]) in sims_dict:\n",
    "                    if sims_dict[(cen_id, ids[1])] > max_sim:\n",
    "                        max_sim = sims_dict[(cen_id, ids[1])]\n",
    "                        max_cen = cen_id\n",
    "            doc_cluster[ids[1]] = max_cen\n",
    "        \n",
    "        #clusters = {}\n",
    "        for doc_id, cen_id in doc_cluster.items():\n",
    "            if cen_id in clusters:\n",
    "                clusters[cen_id].append(doc_id)\n",
    "            else:\n",
    "                clusters[cen_id] = [doc_id]\n",
    "\n",
    "        # calc avg\n",
    "        new_cens = []\n",
    "        for cen_id, doc_ids in clusters.items():\n",
    "            avg_tf = {}\n",
    "            for doc_id in doc_ids:\n",
    "                term_freq = doc_term_freq[doc_id][1]\n",
    "                for t, f in term_freq.items():\n",
    "                    if t in avg_tf:\n",
    "                        avg_tf[t] += (1+math.log(f))/len(clusters[cen_id])\n",
    "                    else:\n",
    "                        avg_tf[t] = (1+math.log(f))/len(clusters[cen_id])\n",
    "            new_cens.append(avg_tf)\n",
    "        initial_centers = new_cens\n",
    "    return initial_centers, clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "def calc_RSS(centers, clusters, center_len):\n",
    "    rss_l = [0 for i in range(len(centers))]\n",
    "    for k,v in clusters.items():\n",
    "        for i in tqdm(v):\n",
    "            for word in set(centers[k].keys()).union(set(docs_links[i].keys())):\n",
    "                rss_l[k] += abs(centers.get(word,0) - docs_links[i].get(word,0))**2\n",
    "        if center_len[k]!=0:\n",
    "            rss_l[k] /= center_len[k]\n",
    "    return sum(rss_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers, clusters = do_kmeans(doc_term_feq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([0, 1, 2, 3, 5, 6, 7, 8, 9, 4])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10124\n",
      "26106\n",
      "2386\n",
      "4900\n",
      "2177\n",
      "314\n",
      "1306\n",
      "994\n",
      "76\n",
      "1150\n"
     ]
    }
   ],
   "source": [
    "for k, v in clusters.items():\n",
    "    print(len(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "N=len(docs_links[\"content\"])\n",
    "def compute_similarity(nd_q, weighted_inv_ind):\n",
    "    similarities = {}\n",
    "    for k,v in nd_q.items():\n",
    "        if k not in weighted_inv_ind:\n",
    "            continue\n",
    "        idf = math.log(N/weighted_inv_ind[k][0])\n",
    "        for docid, w in weighted_inv_ind[k][1].items():\n",
    "            if docid in similarities.keys():\n",
    "                similarities[docid] += idf*w*v\n",
    "            else:\n",
    "                similarities[docid] = idf*w*v\n",
    "    for doc_id, sim in similarities.items():\n",
    "        similarities[doc_id] = sim/docs_len[doc_id]\n",
    "    return similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_center(nd_q, centers, weighted_inv_ind):\n",
    "    similarities = {}\n",
    "    for k,v in nd_q.items():\n",
    "        if k not in weighted_inv_ind:\n",
    "            continue\n",
    "        idf = weighted_inv_ind[k][0]\n",
    "        for i in range(len(centers)):\n",
    "            if k in centers[i]:\n",
    "                if i in similarities:\n",
    "                    similarities[i] += idf*centers[i][k]*v\n",
    "                else:\n",
    "                    similarities[i] = idf*centers[i][k]*v\n",
    "    return max(similarities.items(), key=operator.itemgetter(1))[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_k_most_sim(query, clusters, doc_term_feq, weighted_inv_ind, k=5):\n",
    "    q_tokens = tokenize(None, query)\n",
    "    picked_center = pick_center(q_tokens, centers, weighted_inv_ind)\n",
    "    sims = {}\n",
    "    for t, v in q_tokens.items():\n",
    "        if t not in weighted_inv_ind:\n",
    "            continue\n",
    "        idf = weighted_inv_ind[t][0]\n",
    "        for doc_id in clusters[str(picked_center)]:\n",
    "            #print(doc_id)\n",
    "            doc_terms = doc_term_feq[doc_id][1]\n",
    "            #print(doc_terms)\n",
    "            if t in doc_terms:\n",
    "                if doc_id in sims:\n",
    "                    sims[doc_id] += idf*doc_terms[t]*v\n",
    "                else:\n",
    "                    sims[doc_id] = idf*doc_terms[t]*v\n",
    "    scores = sims\n",
    "    if len(scores) < k:\n",
    "        k = len(scores)\n",
    "    best_scores =[]\n",
    "    heap = []\n",
    "    for docid,score in scores.items():\n",
    "        heapq.heappush(heap,(-score, docid))\n",
    "    for _ in range(k):\n",
    "        best_scores.append(heapq.heappop(heap))\n",
    "    return best_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'int'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(-14.58628306529145, 24911),\n",
       " (-13.949702708906685, 10090),\n",
       " (-13.23003045700133, 10031),\n",
       " (-13.23003045700133, 10043),\n",
       " (-12.036480758470455, 10692)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_k = get_k_most_sim(\"اعزام کاروان های اردوی راهیان نور\", clusters, doc_term_feq, weighted_inv_ind, 5)\n",
    "top_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links(top_k, docs_links):\n",
    "    results=[]\n",
    "    for i in top_k:\n",
    "        results.append(docs_links.loc[docs_links.index[docs_links['id'] == i[1]].tolist()[0]][\"url\"])\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.farsnews.ir/news/13991215000577/تکلیف-اردوهای-راهیان-نور-در-دومین-نوروز-کرونایی-چه-می\\u200cشود',\n",
       " 'https://www.isna.ir/news/98043015977/تشریح-جزئیات-حضور-مردم-اردبیل-در-یادمان-های-شمال-غرب-کشور',\n",
       " 'https://www.isna.ir/news/98041910370/خوزستانی-ها-به-مناطق-عملیاتی-غرب-کشور-می-روند',\n",
       " 'https://www.isna.ir/news/98041910370/خوزستانی-ها-به-مناطق-عملیاتی-غرب-کشور-می-روند',\n",
       " 'https://www.isna.ir/news/98081408553/اعلام-جزئیات-حضور-دانش-آموزان-گیلانی-در-اردوهای-راهیان-نور']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_links(top_k, docs_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "a_file = open(\"clusters.json\", \"w\")\n",
    "a_file = json.dump(clusters, a_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10124\n",
      "26106\n",
      "2386\n",
      "4900\n",
      "2177\n",
      "314\n",
      "1306\n",
      "994\n",
      "76\n",
      "1150\n"
     ]
    }
   ],
   "source": [
    "a_file = open(\"clusters.json\", \"r\")\n",
    "clusters = json.load(a_file)\n",
    "clusters\n",
    "\n",
    "for k, v in clusters.items():\n",
    "    print(len(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'int'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['https://www.farsnews.ir/news/13991105000917/دولت-بازار-سرمایه-را-سیاسی-کرد-فروش-سهام-عدالت-از-سوی-کارگزاری\\u200cها-و',\n",
       " 'https://www.farsnews.ir/news/13990224001102/تامین-مالی-000-میلیارد-تومانی-در-بورس-تهران-مردم-در-2000-نقطه-به-بازار',\n",
       " 'https://www.farsnews.ir/news/13990916000390/سازمان-بورس-آماده-برخورد-سریع-و-قاطع-با-متخلفان-تضعیف-مقام-ناظر-بدعتی',\n",
       " 'https://www.farsnews.ir/news/13991217000737/بورس-ایران-تحلیل-پذیری-خود-را-از-دست-داده-است-ریسک-رمزارزها-برای',\n",
       " 'https://www.farsnews.ir/news/13990528000138/ورود-0-هزار-میلیارد-تومان-نقدینگی-تازه-واردها-به-بازار-سرمایه-مسئولیت']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_k = get_k_most_sim(\"سرمایه گذاری در بازار بورس\", clusters, doc_term_feq, weighted_inv_ind, 5)\n",
    "get_links(top_k, docs_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('mlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4976e0179d97dd6d59b1329a76e601e17b789c2571b41c8b57f5fd69821c0dd3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
